{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install datasets\n",
        "!pip install scikit-learn\n",
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "1YnBKS83dGvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb90242c-a3ea-408f-86d8-f9ec365098af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Collecting xxhash\n",
            "  Using cached xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting multiprocess\n",
            "  Using cached multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Collecting responses<0.19\n",
            "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting dill<0.3.6\n",
            "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Using cached multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed datasets-2.6.1 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=9801862e0f081f21b5f604c296a5024619c43ac47200ff6464c3bccfc48382c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9UGKI7Bzc2MK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import uuid\n",
        "import random\n",
        "import torch\n",
        "from transformers import ElectraTokenizerFast, ElectraModel, Trainer, TrainingArguments, ElectraForTokenClassification\n",
        "from transformers import pipeline\n",
        "from transformers.convert_graph_to_onnx import convert\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from datasets.dataset_dict import Dataset, DatasetDict\n",
        "from datasets import load_dataset, load_metric\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datasets\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "-dbe6yGazoW9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jibsM8Ztc2MO"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"google/electra-small-discriminator\"\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6-He1zEc2MP",
        "outputId": "9202563a-4279-4883-be47-a264823f3329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model =  ElectraForTokenClassification.from_pretrained(model_checkpoint, num_labels=13)\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "MApIJNAcf9LL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c58852-6d08-4cb7-e805-998ab3acf2db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eGZUs8SPc2MQ"
      },
      "outputs": [],
      "source": [
        "f = open(\"train.txt\", \"r\")\n",
        "text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "l66XNJJfc2MR"
      },
      "outputs": [],
      "source": [
        "updated_text = text.split(\"\\n\\t\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "aqrTzKm50kra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "FH2HVnwa0lTt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jzenW5qtc2MR"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for sent in updated_text:\n",
        "    sent_text =  []\n",
        "    sent_tag = []\n",
        "    for i in sent.split(\"\\n\"):\n",
        "        w = i.split(\"\\t\")\n",
        "        if w[-1] != \"\":\n",
        "            sent_text.append(w[0])\n",
        "            sent_tag.append(w[-1])\n",
        "    data.append({\"tokens\":sent_text, \"ner_tags\":sent_tag,\"id\":uuid.uuid4().hex})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "qHKMwypAc2MS",
        "outputId": "09603999-c017-4b92-831c-530a21e20289"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 tokens  \\\n",
              "0     [@paulwalk, It, 's, the, view, from, where, I,...   \n",
              "1     [From, Green, Newsfeed, :, AHFA, extends, dead...   \n",
              "2     [Pxleyes, Top, 50, Photography, Contest, Pictu...   \n",
              "3        [today, is, my, last, day, at, the, office, .]   \n",
              "4     [4Dbling, 's, place, til, monday, ,, party, pa...   \n",
              "...                                                 ...   \n",
              "2390  [Five, Bullpens, to, Stay, Away, From, in, MLB...   \n",
              "2391  [New, Dorky, Stuff, :, Friday, Review, Rundown...   \n",
              "2392  [Oh, PEI, ,, i, always, forget, how, beautiful...   \n",
              "2393  [workin, at, the, ol, Sunset, Bistro, tonight,...   \n",
              "2394  [STOP, WHAT, YOU'RE, DOING, AND, GO, GET, #Exp...   \n",
              "\n",
              "                                               ner_tags  \\\n",
              "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...   \n",
              "1     [O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...   \n",
              "2      [B-corporation, O, O, O, O, O, O, O, O, O, O, O]   \n",
              "3                           [O, O, O, O, O, O, O, O, O]   \n",
              "4           [B-person, O, O, O, O, O, O, O, O, O, O, O]   \n",
              "...                                                 ...   \n",
              "2390  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "2391  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "2392  [O, B-person, O, O, O, O, O, O, O, O, O, O, O,...   \n",
              "2393  [O, O, O, O, B-location, I-location, O, O, O, ...   \n",
              "2394  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "\n",
              "                                    id  \n",
              "0     97e1c448465b485ea7da909352c46c67  \n",
              "1     795e67eddddb488fb5bbcba1cb615ba1  \n",
              "2     9c8169f1298546d4853b3ab6ffadebad  \n",
              "3     ee5d5e4dac9a4426989949ff3e5264ad  \n",
              "4     5b4f7c6e71ad4ef9b85cd76053810fcc  \n",
              "...                                ...  \n",
              "2390  76149cf40e2e47fea76376968bd477a7  \n",
              "2391  c484c56b39cc4088ba66fef8c4520472  \n",
              "2392  38516533fb594387b335188326febefd  \n",
              "2393  a2b0e82d1c9f4a88a5b5a31309ead3ac  \n",
              "2394  419f06c72336460d82f4f8fe87bbb8f0  \n",
              "\n",
              "[2395 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed3ffdbb-be9c-4371-a316-8f91588917ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...</td>\n",
              "      <td>97e1c448465b485ea7da909352c46c67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>795e67eddddb488fb5bbcba1cb615ba1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[B-corporation, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>9c8169f1298546d4853b3ab6ffadebad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>ee5d5e4dac9a4426989949ff3e5264ad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[B-person, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>5b4f7c6e71ad4ef9b85cd76053810fcc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2390</th>\n",
              "      <td>[Five, Bullpens, to, Stay, Away, From, in, MLB...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>76149cf40e2e47fea76376968bd477a7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2391</th>\n",
              "      <td>[New, Dorky, Stuff, :, Friday, Review, Rundown...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>c484c56b39cc4088ba66fef8c4520472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2392</th>\n",
              "      <td>[Oh, PEI, ,, i, always, forget, how, beautiful...</td>\n",
              "      <td>[O, B-person, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>38516533fb594387b335188326febefd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2393</th>\n",
              "      <td>[workin, at, the, ol, Sunset, Bistro, tonight,...</td>\n",
              "      <td>[O, O, O, O, B-location, I-location, O, O, O, ...</td>\n",
              "      <td>a2b0e82d1c9f4a88a5b5a31309ead3ac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2394</th>\n",
              "      <td>[STOP, WHAT, YOU'RE, DOING, AND, GO, GET, #Exp...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>419f06c72336460d82f4f8fe87bbb8f0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2395 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed3ffdbb-be9c-4371-a316-8f91588917ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed3ffdbb-be9c-4371-a316-8f91588917ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed3ffdbb-be9c-4371-a316-8f91588917ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data_df = pd.DataFrame(data)\n",
        "data_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ebdKXjzAc2MT"
      },
      "outputs": [],
      "source": [
        "flat_list = [item for sublist in data_df['ner_tags'] for item in sublist]\n",
        "m = set(flat_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wK5MNZ-Rc2MT"
      },
      "outputs": [],
      "source": [
        "# Creating the DataFrame from the dataset lists\n",
        "train_data_df,val_data_df  = train_test_split(data_df,test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v8RcKhYEc2MU"
      },
      "outputs": [],
      "source": [
        "dataset_info = datasets.DatasetInfo(\n",
        "    description=\"_DESCRIPTION\",\n",
        "    features=datasets.Features(\n",
        "        {\n",
        "            \"id\": datasets.Value(\"string\"),\n",
        "            \"tokens\": datasets.Sequence(datasets.Value(\"string\")),\n",
        "            \"ner_tags\": datasets.Sequence(\n",
        "                datasets.features.ClassLabel(num_classes=13,\n",
        "                    names=sorted(['O', 'B-corporation',\n",
        "                                         'B-creative-work',\n",
        "                                         'B-group',\n",
        "                                         'B-location',\n",
        "                                         'B-person',\n",
        "                                         'B-product',\n",
        "                                         'I-corporation',\n",
        "                                         'I-creative-work',\n",
        "                                         'I-group',\n",
        "                                         'I-location',\n",
        "                                         'I-person',\n",
        "                                         'I-product'\n",
        "                                       ])\n",
        "                                                        )\n",
        "                                                    )\n",
        "                                                }\n",
        "                                            ),\n",
        "                                            supervised_keys=None,\n",
        "                                            homepage=\"\",\n",
        "                                            citation=\"_CITATION\",\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rTmDvDAc2MV",
        "outputId": "c8c6595e-e5e7-4c6e-e7ef-896951a1fca1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetInfo(description='_DESCRIPTION', citation='_CITATION', homepage='', license='', features={'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['B-corporation', 'B-creative-work', 'B-group', 'B-location', 'B-person', 'B-product', 'I-corporation', 'I-creative-work', 'I-group', 'I-location', 'I-person', 'I-product', 'O'], id=None), length=-1, id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "dataset_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QgLPP8uOc2MW"
      },
      "outputs": [],
      "source": [
        "def map_df_rows(df):\n",
        "    \"\"\"\n",
        "    Function to map DataFrame Label row with integers, The dictionary given in below\n",
        "    function can be updated as per the requirments\n",
        "    Args:\n",
        "        df: Input DataFrame for which we need to transform ner_tags label\n",
        "    Returns:\n",
        "        List of mapped integers with labels\n",
        "    \"\"\"\n",
        "    updated_rows = []\n",
        "    # The following map dict can be updated if more annotations comes in\n",
        "    map_df_rows_dict = {\n",
        "                         \"O\":0,\n",
        "                         \"B-corporation\":1,\n",
        "                         \"B-creative-work\":2,\n",
        "                         \"B-group\":3,\n",
        "                         \"B-location\":4,\n",
        "                         \"B-person\":5,\n",
        "                         \"B-product\":6,\n",
        "                         \"I-corporation\":7,\n",
        "                         \"I-creative-work\":8,\n",
        "                         \"I-group\":9,\n",
        "                         \"I-location\":10,\n",
        "                         \"I-person\":11,\n",
        "                         \"I-product\":12}\n",
        "    for row in list(df['ner_tags']):\n",
        "        updated_rows.append(list(map(map_df_rows_dict.get, row)))\n",
        "\n",
        "    return updated_rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7o8keLPlc2MX"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    \"\"\"\n",
        "    Function for tokenizing and aligning labels\n",
        "    \"\"\"\n",
        "    #tokenizer = self.artifacts.ner_model.get(\"tokenizer\")\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True,max_length=512, is_split_into_words=True)\n",
        "    print (\"tokenized_inputs\", tokenized_inputs.keys())\n",
        "    labels = []\n",
        "    task = \"ner\"\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        \n",
        "        labels.append(label_ids)\n",
        "\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    tokenized_inputs[\"tokens\"] = examples[\"tokens\"]\n",
        "    tokenized_inputs[\"id\"] = examples[\"id\"]\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jEb0rzXguHP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "5b3e7f9c22514158bd98648462b7f5c6",
            "aec8089482f94908a338eec012f929f1",
            "41403e95d81f4ddc9e98025478c0981c",
            "99fa2e7ec5e448e0840ba7ccd0365d7c",
            "c625f6dabe2440c0b9c948301e9cdd2f",
            "cae9e256d7ff47c1a46d0a92b0b59acc",
            "4b1f148e68234cc5a8ef7780b4077324",
            "63feafd108db45e6814f0d368ba675f9",
            "868d56759199411297904f861f962b68",
            "234bc2ff31af41698a5a984e48f11fc8",
            "752017550ea04c82890eee47247bd3f2"
          ]
        },
        "id": "30GMEKklc2MX",
        "outputId": "2b3b9562-cc79-4d52-ead4-944b67a0ba1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b3e7f9c22514158bd98648462b7f5c6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "metric = load_metric(\"seqeval\")\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "t8GD5hq_c2MY",
        "outputId": "7e19535c-cd2f-47dc-a74a-232ee2cf07f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 tokens  \\\n",
              "1540  [@jeffstinco, or, @chuckcomeau, follow, me, fo...   \n",
              "1579  [I, had, a, good, day, :), ,, how, y'all, doin...   \n",
              "829   [Not, awful, ..., going, to, start, fresh, thi...   \n",
              "441   [RT, @askyfullofstars, :, AstronomersWithoutBo...   \n",
              "1361  [WHATS, GOODIE, EVERY1, COOLIN, COOLIN, FEELIN...   \n",
              "\n",
              "                                               ner_tags  \\\n",
              "1540  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "1579                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
              "829             [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
              "441   [O, O, O, B-group, O, O, O, O, O, O, O, O, O, ...   \n",
              "1361                        [O, O, O, O, O, O, O, O, O]   \n",
              "\n",
              "                                    id  \n",
              "1540  ccfe5bc2fb554982b8b1d06868b32869  \n",
              "1579  14083152097847ecb1b0d5716636e744  \n",
              "829   afa8bef331fc4a85b58d52f5fb28fb5e  \n",
              "441   ad51c6fe70954b48863c326281aeabd2  \n",
              "1361  8a0eaec6c0104448a88800da52b1b298  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1605afa2-3811-4745-9f80-2f94688755a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1540</th>\n",
              "      <td>[@jeffstinco, or, @chuckcomeau, follow, me, fo...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>ccfe5bc2fb554982b8b1d06868b32869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1579</th>\n",
              "      <td>[I, had, a, good, day, :), ,, how, y'all, doin...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>14083152097847ecb1b0d5716636e744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>[Not, awful, ..., going, to, start, fresh, thi...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>afa8bef331fc4a85b58d52f5fb28fb5e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>[RT, @askyfullofstars, :, AstronomersWithoutBo...</td>\n",
              "      <td>[O, O, O, B-group, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>ad51c6fe70954b48863c326281aeabd2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>[WHATS, GOODIE, EVERY1, COOLIN, COOLIN, FEELIN...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>8a0eaec6c0104448a88800da52b1b298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1605afa2-3811-4745-9f80-2f94688755a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1605afa2-3811-4745-9f80-2f94688755a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1605afa2-3811-4745-9f80-2f94688755a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "val_data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "xVinUYyuc2MY",
        "outputId": "e2440318-c75c-4ab3-d492-f67dd2f820b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 tokens  \\\n",
              "1309  [@CHRIS_Daughtry, Aw, ur, cute, when, u, get, ...   \n",
              "498   [check, out, @140hours, auction, starts, tonig...   \n",
              "2195  [@DesiiDanii4, nou, ze, is, al, een, week, bes...   \n",
              "740   [No, Good, Punk, :, Thug, knocks, 84-year-old,...   \n",
              "2077  [@ahmong, I'm, thinking, they, trade, him, bef...   \n",
              "\n",
              "                                               ner_tags  \\\n",
              "1309  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "498                      [O, O, O, O, O, O, O, O, O, O]   \n",
              "2195   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
              "740       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
              "2077  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "\n",
              "                                    id  \n",
              "1309  e0a7745bd692468f9179064f6a1a9923  \n",
              "498   e094a8934c954e09854667fc89f45723  \n",
              "2195  5dcae5d1797d438cabf3303bd175d82e  \n",
              "740   5a953a1360aa4e37a3e893ad98e32e7f  \n",
              "2077  c4b38db659694d809dd3493f4933071a  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65caf64a-cc61-41b6-bf1a-a3d42e1e54d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>[@CHRIS_Daughtry, Aw, ur, cute, when, u, get, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>e0a7745bd692468f9179064f6a1a9923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>[check, out, @140hours, auction, starts, tonig...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>e094a8934c954e09854667fc89f45723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2195</th>\n",
              "      <td>[@DesiiDanii4, nou, ze, is, al, een, week, bes...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>5dcae5d1797d438cabf3303bd175d82e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>[No, Good, Punk, :, Thug, knocks, 84-year-old,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>5a953a1360aa4e37a3e893ad98e32e7f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2077</th>\n",
              "      <td>[@ahmong, I'm, thinking, they, trade, him, bef...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>c4b38db659694d809dd3493f4933071a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65caf64a-cc61-41b6-bf1a-a3d42e1e54d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65caf64a-cc61-41b6-bf1a-a3d42e1e54d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65caf64a-cc61-41b6-bf1a-a3d42e1e54d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "train_data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vB_TFP9Oc2MY"
      },
      "outputs": [],
      "source": [
        "val_data_df['ner_tags'] = map_df_rows(val_data_df)\n",
        "train_data_df['ner_tags'] = map_df_rows(train_data_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "umW4_mKlc2MZ"
      },
      "outputs": [],
      "source": [
        "# Map the Labels with integers\n",
        "\n",
        "train_data = Dataset.from_pandas(train_data_df,info= dataset_info, preserve_index=False)\n",
        "val_data = Dataset.from_pandas(val_data_df,info= dataset_info, preserve_index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptOx4D8Zc2MZ",
        "outputId": "3a2aec56-db3d-4a81-9d60-d6f16fea2d96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-corporation',\n",
              " 'B-creative-work',\n",
              " 'B-group',\n",
              " 'B-location',\n",
              " 'B-person',\n",
              " 'B-product',\n",
              " 'I-corporation',\n",
              " 'I-creative-work',\n",
              " 'I-group',\n",
              " 'I-location',\n",
              " 'I-person',\n",
              " 'I-product',\n",
              " 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "dataset_dict = DatasetDict({\"train\": train_data, 'val': val_data})\n",
        "label_list = dataset_dict[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnY3OG18c2MZ",
        "outputId": "3bcc8545-3240-4020-a789-1ca575de76d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1916\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 479\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "9988b2ab199549d8af175c7ab5cb794a",
            "0bedfe5103854e57adebf720143e21fd",
            "6b6202d3fa5d462a8bca5ae3aa89c482",
            "c0be6ed22ce7444fbf6c0acc8a3b8cd1",
            "5ca3d45dc76b407db10066b0cf685cb1",
            "d5c677a5eee241c08798fa0e4fec6133",
            "cf9f3e0aa87b47a59c3cc684ddaa9b15",
            "365b684eced14b149b00a223886a4866",
            "ad929dd29e464817b8e9a53ed3aaf285",
            "9d33f82d50bb482f88a106252fdbf7cc",
            "39f4f8e626ef4c8885d09c901fe52f17",
            "1695e6497ac1471780a914519e88b7b4",
            "dda008d41825416da96d9d11930c5394",
            "c3b618ccefb24449bf25c9ae8d3e3c19",
            "45b010e5ee3e4ef4b089d5a0d4d26d4c",
            "ee1b85ee15644e738351cfeb0c62efe1",
            "200cf109007a4a1890c1de28e3a227e1",
            "7aca39f9474841998e8133c6e5ff3450",
            "ba9673ef17354844a6853a57ce3159b8",
            "ab3d2c361cf9475eb31063cfc29c0a9b",
            "62052ebc800347c68169bf28cd647748",
            "13ce58fc81da40be8e75b83a5089bc5f"
          ]
        },
        "id": "pKRD9k7Zc2Ma",
        "outputId": "530be7e9-dd97-4051-8674-474fb6386561"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9988b2ab199549d8af175c7ab5cb794a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized_inputs dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "tokenized_inputs dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1695e6497ac1471780a914519e88b7b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized_inputs dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
          ]
        }
      ],
      "source": [
        "task = \"ner\"\n",
        "label_all_tokens = True\n",
        "# Tokenize the dataset context        \n",
        "tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True, remove_columns = dataset_dict[\"train\"].column_names)\n",
        "# Define Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhSqs20fc2Ma",
        "outputId": "0ccd7a5b-ee6b-4985-989b-35d76083382f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': InMemoryTable\n",
              " id: string\n",
              " tokens: list<item: string>\n",
              "   child 0, item: string\n",
              " input_ids: list<item: int32>\n",
              "   child 0, item: int32\n",
              " token_type_ids: list<item: int8>\n",
              "   child 0, item: int8\n",
              " attention_mask: list<item: int8>\n",
              "   child 0, item: int8\n",
              " labels: list<item: int64>\n",
              "   child 0, item: int64\n",
              " ----\n",
              " id: [[\"e0a7745bd692468f9179064f6a1a9923\",\"e094a8934c954e09854667fc89f45723\",\"5dcae5d1797d438cabf3303bd175d82e\",\"5a953a1360aa4e37a3e893ad98e32e7f\",\"c4b38db659694d809dd3493f4933071a\",\"9aab433eee7141eb839a80c3f1f440d8\",\"92d740eeed5a4eb8878bd8039b8974a9\",\"940a0392a9e641398c6f05c0003b36f6\",\"d88956fed481493d9f97bafd240d68fc\",\"2256343096d04a718de3fcd0e5f51b10\",...,\"cf0e2957753a49cba5b698fb44e92740\",\"20f5caa183954379b63f618952e55aa4\",\"95df1f26e5484a4580bc5bbd217dd4db\",\"94fbf73f76ae43508f81a8c572d2a941\",\"0d4ffbcc0de4415c8abee37556225a2c\",\"e3e8a3c103c749e598a3fe75de07ffbe\",\"b48fb7ab588b495dbebba3b8f4ac0049\",\"add237cc24f449d69f22a5122377c81a\",\"362d435c726b4d44bdd2087a139b8521\",\"420e70377dac4a9b8ccb56483d4727c1\"],[\"52c8b2efdd7d4b14afc0455f9bf90de7\",\"631d04a79435468d90778d941da6b95e\",\"0d71108bf53f40bcb4bc3b81a10d43de\",\"e43a6c52576449bea1ffe4c9b303142e\",\"122b03144ad74565bd65e434dc7e3eb9\",\"f87aea003e094879892625d4b713d839\",\"79f8acdb0f2e4d38ada664f44de186c3\",\"f05b762ef7214cb4820cc3694ba742ec\",\"24ab31c42cb948268522b4b1bc4e7c14\",\"d2ec0532bc11461ea1f00dda7136e1ae\",...,\"e1dbdb9e8a3b4eb5993419abf16a8b17\",\"91a4d5ad16074a4e85b333f8a10fd2b1\",\"73692c14c6fd49828cbced265c3d25d6\",\"215bbc5f0adb4c6083397c1f3a3344de\",\"2b1a29c834d84f21a4f91a2721810b3f\",\"8483695e84044c9b9226c17f20cde85e\",\"7082c326dd2e4899adfe22bf3730a914\",\"2160df98ec8a43ae88f9de0f73824426\",\"e2e1292cc9af40e7b4eced150d5f7fe3\",\"b9f00b683338491594c8cf87ac2e8713\"]]\n",
              " tokens: [[[\"@CHRIS_Daughtry\",\"Aw\",\"ur\",\"cute\",\"when\",\"u\",\"get\",\"mad\",\"ur\",\"songs\",...,\"bros\",\"do\",\"which\",\"is\",\"weird\",\"they\",\"don't\",\"like\",\"my\",\"stuff\"],[\"check\",\"out\",\"@140hours\",\"auction\",\"starts\",\"tonight\",\"!\",\"Dreaming\",\"Storm\",\"http://bit.ly/5Bq0Qm\"],[\"@DesiiDanii4\",\"nou\",\"ze\",\"is\",\"al\",\"een\",\"week\",\"best\",\"wel\",\"erg\",\"ziek\",\",\",\"echt\",\"heel\",\"zielig\",\"!:\"],[\"No\",\"Good\",\"Punk\",\":\",\"Thug\",\"knocks\",\"84-year-old\",\"to\",\"the\",\"ground\",\",\",\"steals\",\"her\",\"purse\",\"http://lsnlw.com/t/4188153892/\"],[\"@ahmong\",\"I'm\",\"thinking\",\"they\",\"trade\",\"him\",\"before\",\"the\",\"deadline\",\"and\",...,\"it\",\"up\",\"so\",\"they\",\"don't\",\"lose\",\"him\",\"for\",\"nothing\",\".\"],[\"Massive\",\"B-stock\",\"list\",\"!\",\"While\",\"supplies\",\"last\",\".\",\"Check\",\"it\",\"out\",\"here\",\":\",\"http://soniccircus.com/September-B-Stock-List\"],[\"Institutional\",\"buying\",\"lifts\",\"UFlex\",\"30%\",\"in\",\"a\",\"month\",\"http://bit.ly/bOAk9y\"],[\"It\",\"'s\",\"official\",\".\",\"Next\",\"book\",\"club\",\"selection\",\"is\",\"\"\",...,\"show\",\"a\",\"few\",\"years\",\"ago\",\"and\",\"it\",\"was\",\"fantastic\",\".\"],[\"RT\",\"@GdnPolitics\",\":\",\"RT\",\"@AlJahom\",\":\",\"Blair\",\":\",\"\"\",\"I'm\",...,\"as\",\"you\",\"make\",\"sure\",\"it\",\"'s\",\"my\",\"turn\",\"next\",\"\"\"],[\"starting\",\"to\",\"be\",\"temped\",\"to\",\"get\",\"an\",\"android\",\"device\",\",\",\"but\",\"prob\",\"next\",\"gen\",\"after\",\"desire/desirehd/galaxy\",\"s\",\".\",\"critical\",\"mass/commercial\"],...,[\"Recording\",\"vocals\",\"tonight\",\"!\",\"Can't\",\"wait\",\"for\",\"the\",\"song\",\"to\",\"be\",\"done\",\"!\"],[\"watching\",\"the\",\"VMAs\",\"Eminem\",\"is\",\"BeAsT\",\"and\",\"is\",\"smexy\",\"!!;\",...,\"got\",\"a\",\"boyfren\",\"he\",\"gave\",\"me\",\"the\",\"cutest\",\"thing\",\"today\"],[\"REFINERY29\",\"STALKS\",\"LA'S\",\"MOST\",\"FASHIONABLE\",\"FEMMES\",\"FROM\",\"LAST\",\"WEEK\",\"VB'S\",\"PARISIAN\",\"BREAKFAST\",\".\",\"MERCI\",\",\",\"MERCI\",\"!\",\"XX\",\"http://ow.ly/2G4KX\"],[\"'I\",\"just\",\"took\",\"\"\",\"After\",\"getting\",\"trampled\",\"at\",\"a\",\"Justin\",...,\"got\",\":\",\"Part\",\"1\",\":)\",\"!\",\"Try\",\"it\",\":\",\"http://tinyurl.com/3alx5up'\"],[\"George\",\"N\",\".\",\"Parks\",\",\",\"UMass\",\"band\",\"director\",\",\",\"dies\",...,\".\",\"Parks\",\",\",\"for\",\"33\",\"years\",\"the\",\"dire\",\"...\",\"http://tinyurl.com/2femvgq\"],[\"@atlgc17\",\"Im\",\"16\",\"and\",\"a\",\"freshmen\",\"again\",\".\",\"I\",\"fluncked\",...,\"fluncked\",\"last\",\"year\",\"because\",\"I\",\"never\",\"did\",\"my\",\"work\",\".\"],[\"@ConorMc_Ginty\",\"GaGa\",\"played\",\"here\",\"a\",\"few\",\"weeks\",\"back\",\".\",\"She\",...,\"during\",\"the\",\"show\",\".\",\"God\",\"bless\",\"her\",\"for\",\"that\",\".\"],[\"What\",\"my\",\"#Followers\",\"Doing\",\"tonight\",\"?\"],[\"@valdary\",\"sorry\",\"i\",\"meant\",\"my\",\"dad\",\"in\",\"the\",\"first\",\"tweet\",\",\",\"my\",\"mums\",\"been\",\"dead\",\"5\",\"years\",\".\"],[\"@divacoachdabney\",\"you\",\"made\",\"Nina\",\"'s\",\"day\",\".\",\"Sweet\",\"surprises\",\"for\",\"you\",\".\"]],[[\"RT\",\"@BettyCrocker\",\":\",\"Less\",\"than\",\"25\",\"followers\",\"to\",\"go\",\"!\",...,\"'s\",\"reach\",\"11,000\",\"before\",\"the\",\"weekend\",\"!\",\":)\",\"http://ht.ly/2G4sQ\",\"#sweepstakes\"],[\"@lofolulu\",\"RT\",\"@couponprincess\",\":\",\"Please\",\"vote\",\"for\",\"me\",\"to\",\"be\",...,\"November\",\"!\",\"TY\",\"http://bit.ly/cgmnoX\",\"-\",\"DO\",\"WHAT\",\"@LOFOLULU\",\"SAYS\",\"!\"],[\"@PaulJones3\",\"delicioso\",\".\",\"hows\",\"your\",\"week\",\"?\"],[\"overheard\",\"on\",\"subway\",\"...\"\",\"so\",\"when\",\"you\",\"run\",\",\",\"do\",\"your\",\"elbows\",\"sweat\",\"?\"\"],[\"What\",\"Do\",\"S.F.\",\"Rabbis\",\"Eat\",\"Before\",\"and\",\"After\",\"Yom\",\"Kippur\",...,\"We\",\"Asked\",\"Five\",\"to\",\"Enlighten\",\"Us\",\"!\",\"via\",\"@SFoodie\",\"http://bit.ly/aQrFUz\"],[\"what\",\"happens\",\"when\",\"your\",\"happy\",\"but\",\"heartbroken\",\"?\",\"this\",\".\"],[\"Done\",\"teaching\",\"in\",\"Viera\",\"time\",\"for\",\"some\",\"eats\",\"and\",\"back\",\"to\",\"orlando\",\",\",\"possibly\",\"swing\",\"dance\",\"tonight\",\"!\"],[\"[\",\"GigaOM\",\"]\",\"DRM\",\"FAIL\",\":\",\"Five\",\"Broken\",\"Copy\",\"Protection\",...,\"we\",\"learned\",\"that\",\"the\",\"HDCP\",\"copy\",\"protection\",\"scheme\",\"...\",\"http://bit.ly/blWf0e\"],[\"holy\",\"cow\",\"-\",\"I'm\",\"almost\",\"ready\",\"for\",\"sessions\",\"!\",\"Walls\",...,\",\",\"backdrop\",\"system\",\"up\",\"and\",\"working\",\"!\",\"Tomorrow\",\"...\",\"http://fb.me/t8hzqbAZ\"],[\"Good\",\"start\",\"for\",\"sharks\",\"tonight\",\"can\",\"only\",\"get\",\"better\",\".\",...,\"Rocks\",\"well\",\"in\",\"game\",\"early\",\"though\",\"until\",\"'\",\"unseen\",\"'\"],...,[\"@themayorpete\",\"I\",\"split\",\"my\",\"time\",\"now\",\"btwn\",\"SF\",\"and\",\"San\",\"Mateo\"],[\"@KFSH\",\"totally\",\"!\",\"I\",\"can\",\"go\",\"a\",\"week\",\"w/o\",\"tv\",...,\"myself\",\"and\",\"God\",\".\",\"Brings\",\"peace\",\"into\",\"my\",\"life\",\".\"],[\"ooo\",\"@claireymarsh\",\"where\",\"are\",\"you\",\"when\",\"i\",\"need\",\"you\",\"..\"],[\"Lmao\",\"..\",\"They\",\"#Hurrt\",\"!\",\"RT\",\"@_sofucKENNrude\",\"y\",\"every\",\"nigga\",...,\"a\",\"long\",\"time\",\"ago\",\".\",\"-_-\",\"skimp\",\"ass\",\"braids\",\".\"],[\"Lingerie\",\"football\",\"league\",\"come\",\"on\",\"tonight\"],[\"RT\",\"@ArianaGrande\",\":\",\"Follow\",\"Friday\",\"@lushcosmetics\",\"...\",\"Just\",\"got\",\"some\",\"yummyyyyy\",\"bubble\",\"bath\",\"goodies\",\"there\",\".\",\":)\",\"xxxxxx\",\"#FF\"],[\"@BiteMe_iimKeema\",\"OMG\",\"...\",\"BUT\",\"HOW\",\"LONG\",\"AGO\",\"U\",\"GOING\",\"OUT\",\"TONIGHT\"],[\"Salem\",\"Daily\",\"Dish//\",\"Chef\",\"'s\",\"Choice\",\"Menu\",\"at\",\"Spoons\",\"NW\",...,\"for\",\"tonight\",\"...:\",\"Caramalized\",\"Onion\",\"Tarthousemade\",\"tart\",\"wit\",\"...\",\"http://bit.ly/98QmI9\"],[\"Beautiful\",\"day\",\"in\",\"Chicago\",\"!\",\"Nice\",\"to\",\"get\",\"away\",\"from\",\"the\",\"Florida\",\"heat\",\".\"],[\"Last\",\"stop\",\"of\",\"the\",\"day\",\"thank\",\"goddddd\",\"(@\",\"H-E-B\",\"Plus\",\")\",\"http://4sq.com/7RDhgd\"]]]\n",
              " input_ids: [[[101,1030,3782,1035,4830,18533,2854,22091,24471,10140,...,2003,6881,2027,2123,1005,1056,2066,2026,4933,102],[101,4638,2041,1030,8574,6806,9236,10470,4627,3892,...,1048,2100,1013,1019,2497,4160,2692,4160,2213,102],[101,1030,4078,6137,7847,6137,2549,2053,2226,27838,...,1010,14925,11039,12073,1062,9257,8004,999,1024,102],[101,2053,2204,7196,1024,26599,21145,6391,1011,2095,...,1013,4601,2620,2620,16068,22025,2683,2475,1013,102],[101,1030,6289,8202,2290,1045,1005,1049,3241,2027,...,2027,2123,1005,1056,4558,2032,2005,2498,1012,102],[101,5294,1038,1011,4518,2862,999,2096,6067,2197,...,4012,1013,2244,1011,1038,1011,4518,1011,2862,102],[101,12148,9343,13695,1057,21031,2595,2382,1003,1999,...,2978,1012,1048,2100,1013,8945,4817,2683,2100,102],[101,2009,1005,1055,2880,1012,2279,2338,2252,4989,...,1037,2261,2086,3283,1998,2009,2001,10392,1012,102],[101,19387,1030,1043,2094,16275,10893,14606,1024,19387,...,2191,2469,2009,1005,1055,2026,2735,2279,1000,102],[101,3225,2000,2022,8915,8737,2098,2000,2131,2019,...,14945,1013,9088,1055,1012,4187,3742,1013,3293,102],...,[101,3405,2955,3892,999,2064,1005,1056,3524,2005,1996,2299,2000,2022,2589,999,102],[101,3666,1996,1058,9335,12495,25832,2003,6841,1998,...,2368,2002,2435,2033,1996,10140,3367,2518,2651,102],[101,21034,24594,29594,2474,1005,1055,2087,19964,26893,...,1012,1048,2100,1013,1016,2290,2549,2243,2595,102],[101,1005,1045,2074,2165,1000,2044,2893,12517,21132,...,1012,4012,1013,23842,2140,2595,2629,6279,1005,102],[101,2577,1050,1012,6328,1010,8529,12054,2316,2472,...,1012,4012,1013,1016,7959,2213,2615,2290,4160,102],[101,1030,2012,2140,18195,16576,10047,2385,1998,1037,...,2197,2095,2138,1045,2196,2106,2026,2147,1012,102],[101,1030,20545,12458,1035,18353,3723,23332,2209,2182,...,1996,2265,1012,2643,19994,2014,2005,2008,1012,102],[101,2054,2026,1001,8771,2725,3892,1029,102],[101,1030,11748,7662,2100,3374,1045,3214,2026,3611,...,1010,2026,12954,2015,2042,2757,1019,2086,1012,102],[101,1030,25992,3597,6776,2850,24700,3240,2017,2081,...,1005,1055,2154,1012,4086,20096,2005,2017,1012,102]],[[101,19387,1030,9306,26775,7432,2121,1024,2625,2084,...,1016,2290,2549,2015,4160,1001,26981,15166,2015,102],[101,1030,8840,14876,7630,7630,19387,1030,8648,2239,...,2079,2054,1030,8840,14876,7630,7630,2758,999,102],[101,1030,2703,14339,2229,2509,3972,27113,6499,1012,2129,2015,2115,2733,1029,102],[101,20443,2006,10798,1012,1012,1012,1000,2061,2043,2017,2448,1010,2079,2115,13690,7518,1029,1000,102],[101,2054,2079,1055,1012,1042,1012,25602,4521,2077,...,2978,1012,1048,2100,1013,1037,4160,12881,17040,102],[101,2054,6433,2043,2115,3407,2021,2540,29162,1029,2023,1012,102],[101,2589,4252,1999,20098,2527,2051,2005,2070,20323,...,2067,2000,10108,1010,4298,7370,3153,3892,999,102],[101,1031,15453,7113,2213,1033,2852,2213,8246,1024,...,1048,2100,1013,1038,2140,2860,2546,2692,2063,102],[101,4151,11190,1011,1045,1005,1049,2471,3201,2005,...,2033,1013,1056,2620,2232,2480,4160,3676,2480,102],[101,2204,2707,2005,12004,3892,2064,2069,2131,2488,...,2092,1999,2208,2220,2295,2127,1005,16100,1005,102],...,[101,1030,2068,28852,14536,12870,1045,3975,2026,2051,2085,18411,7962,16420,1998,2624,19327,102],[101,1030,1047,10343,2232,6135,999,1045,2064,2175,...,1998,2643,1012,7545,3521,2046,2026,2166,1012,102],[101,1051,9541,1030,6249,24335,11650,2232,2073,2024,2017,2043,1045,2342,2017,1012,1012,102],[101,1048,2863,2080,1012,1012,2027,1001,15876,12171,...,1011,1035,1011,8301,8737,4632,24148,2015,1012,102],[101,26577,2666,2374,2223,2272,2006,3892,102],[101,19387,1030,9342,2532,17643,13629,1024,3582,5958,...,2045,1012,1024,1007,22038,20348,20348,1001,21461,102],[101,1030,6805,4168,1035,2462,2213,20553,2863,18168,...,1012,2021,2129,2146,3283,1057,2183,2041,3892,102],[101,10389,3679,9841,1013,1013,10026,1005,1055,3601,...,2978,1012,1048,2100,1013,5818,4160,4328,2683,102],[101,3376,2154,1999,3190,999,3835,2000,2131,2185,2013,1996,3516,3684,1012,102],[101,2197,2644,1997,1996,2154,4067,2643,14141,14141,...,2015,4160,1012,4012,1013,1021,4103,25619,2094,102]]]\n",
              " token_type_ids: [[[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],...,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0]],[[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],...,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0]]]\n",
              " attention_mask: [[[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],...,[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],...,[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1]]]\n",
              " labels: [[[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,1,1,1,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],...,[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,5,5,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,5,11,11,11,0,1,1,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,5,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100]],[[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,4,4,4,4,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,4,4,0,0,0,0,...,0,0,4,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,3,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],...,[-100,0,0,0,0,0,0,0,0,0,0,0,0,4,0,4,10,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,5,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,3,3,9,9,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,4,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,4,0,0,0,0,0,0,0,4,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100]]],\n",
              " 'val': InMemoryTable\n",
              " id: string\n",
              " tokens: list<item: string>\n",
              "   child 0, item: string\n",
              " input_ids: list<item: int32>\n",
              "   child 0, item: int32\n",
              " token_type_ids: list<item: int8>\n",
              "   child 0, item: int8\n",
              " attention_mask: list<item: int8>\n",
              "   child 0, item: int8\n",
              " labels: list<item: int64>\n",
              "   child 0, item: int64\n",
              " ----\n",
              " id: [[\"ccfe5bc2fb554982b8b1d06868b32869\",\"14083152097847ecb1b0d5716636e744\",\"afa8bef331fc4a85b58d52f5fb28fb5e\",\"ad51c6fe70954b48863c326281aeabd2\",\"8a0eaec6c0104448a88800da52b1b298\",\"8de2408aae4c4196b429c1cf9fff4d83\",\"afb8957eadcb4972860d43059fd3e0d5\",\"b27a0b7f6a694c26be41a1a1888a4a65\",\"48ca54bd74104b91b5bf8bbed34a6109\",\"58030a524fcb4a39baab0fc641539010\",...,\"2d932729b03b45c6b0921e3b56edc157\",\"8ab75866c9b0475ab628a5666d0d2b4e\",\"5fd778388ef744adb113f18cb3213e1c\",\"b5944d2262c14aa8b9c5f61d7a4e4666\",\"8c885ad874614842a03f5805070ed64d\",\"30f56a56f84047d7ba794e050d77e7fb\",\"a82f76396469488096ef2a1be79b0c25\",\"c4b4625803444bcb9eeaf02df0695611\",\"48fcdbb9aa284d73a8be61f0a906a29b\",\"da537fca4da24b87886b63df54b94cfe\"]]\n",
              " tokens: [[[\"@jeffstinco\",\"or\",\"@chuckcomeau\",\"follow\",\"me\",\"for\",\"make\",\"this\",\"bad\",\"day\",\"a\",\"HAPPY\",\"day\",\":D\",\"love\",\"you\",\"guys\",\"!!!\"],[\"I\",\"had\",\"a\",\"good\",\"day\",\":)\",\",\",\"how\",\"y'all\",\"doing\",\"?\"],[\"Not\",\"awful\",\"...\",\"going\",\"to\",\"start\",\"fresh\",\"this\",\"week\",\":-)\",\"Soo\",\"hungry\",\"!\"],[\"RT\",\"@askyfullofstars\",\":\",\"AstronomersWithoutBorders\",\"has\",\"a\",\"list\",\"of\",\"live\",\"#InOMN10\",\"broadcasters\",\"for\",\"online\",\"Moon\",\"viewing\",\"Sept\",\"18\",\"http://is.gd/ffDFE\"],[\"WHATS\",\"GOODIE\",\"EVERY1\",\"COOLIN\",\"COOLIN\",\"FEELIN\",\"THE\",\"BREEZE\",\"#FRIDAY#MANIA\"],[\"@_Kinghoopa\",\"the\",\"one\",\"thats\",\"out\",\"now\",\"is\",\"free\",\"...\",\"Father\",...,\"tha\",\"Game\",\"is\",\"droppin\",\"in\",\"a\",\"month\",\"or\",\"so\",\"..\"],[\"@jdub79\",\"We\",\"must\",\"have\",\"been\",\"in\",\"the\",\"same\",\"bathroom\",\"today\",\".\",\"#losers\",\"#youusedtoknowhowtoflush\",\"#followthrough\"],[\"Not\",\"feeling\",\"up\",\"to\",\"par\",\"today\",\".\",\"#stressing\"],[\"RT\",\"@Leonsays\",\":\",\"Need\",\"a\",\"nice\",\"chill\",\"spot\",\"tonight\",\",\",\"no\",\"hype\",\"...\",\"Any\",\"suggestions\",\"?\"],[\"time\",\"to\",\"get\",\"some\",\"sleep\",\".\",\"been\",\"out\",\"from\",\"4pm\",\"till\",\"now\",\".\"],...,[\"#Offline\",\"For\",\"along\",\"time\",\"...\",\"maybe\",\"not\",\"until\",\"tomorrow\"],[\"RT\",\"@mari_dj\",\":\",\"e\",\"nem\",\"joguei\",\"direito\",\".\",\"no\",\"outro\",\"time\",\"estava\",\"o\",\"monstro\",\"do\",\"vitor\",\"D\",\":\",\"ushuahsahs\"],[\"@Strigy\",\"got\",\"mine\",\"in\",\"bbt\",\"aintree\",\"today\",\".\",\"Played\",\"table\",...,\"V\",\"impressed\",\".\",\"Did\",\"you\",\"get\",\"analogue\",\"controller\",\"2\",\"?\"],[\"PeopleSearchAffiliates\",\".\",\"com\",\":\",\"#1\",\"For\",\"Over\",\"2\",\"Years\",\":\",...,\"Most\",\"lucrative\",\"people\",\"search\",\"products\",\"on\",\"CB\",\"for\",\"...\",\"http://bit.ly/aVU9RU\"],[\"LADY\",\"GAGA\",\"IS\",\"BETTER\",\"THE\",\"5th\",\"TIME\",\"OH\",\"BABY(\",\":\"],[\"found\",\"some\",\"really\",\"funny\",\"pictures\",\"of\",\"1st\",\"year\",\":\",\"L\",\"going\",\"to\",\"put\",\"them\",\"on\",\"facebook\",\"now\",\"!!\"],[\"@modernpaper\",\"@alamodestuff\",\"what\",\"a\",\"fabulous\",\"photo\",\"of\",\"you\",\"two\",\"!\",...,\"were\",\"fast\",\"friends\",\",\",\"and\",\"had\",\"an\",\"amazing\",\"time\",\".\"],[\"realy\",\"bored\",\"smack\",\"down\",\"tonite\",\"yay\",\"!!\",\"(\",\":\",\"mood\",\":\",\"bored-_-\",\"ha\",\"!\"],[\"@playavolta\",\"haha\",\"it\",\"'s\",\"not\",\"that\",\"bad\",\"now\",\"but\",\"when\",...,\"i\",\"would\",\"use\",\"the\",\"chewables\",\"that\",\"tasted\",\"like\",\"chalk\",\".\"],[\"now\",\"my\",\"instructor\",\"says\",\"that\",\"when\",\"he\",\"picks\",\"me\",\"up\",...,\"!\",\":\",\"L\",\"I\",\"think\",\"I'll\",\"cope\",\"!!\",\"lol\",\"x\"]]]\n",
              " input_ids: [[[101,1030,5076,16643,15305,2030,1030,8057,9006,10207,...,2154,1024,1040,2293,2017,4364,999,999,999,102],[101,1045,2018,1037,2204,2154,1024,1007,1010,2129,1061,1005,2035,2725,1029,102],[101,2025,9643,1012,1012,1012,2183,2000,2707,4840,2023,2733,1024,1011,1007,17111,7501,999,102],[101,19387,1030,3198,2100,3993,4135,10343,7559,2015,...,1013,2003,1012,1043,2094,1013,21461,20952,2063,102],[101,2054,2015,2204,2666,2296,2487,4658,2378,4658,2378,2514,2378,1996,9478,1001,5958,1001,29310,102],[101,1030,1035,2332,6806,29477,1996,2028,2008,2015,...,4530,8091,1999,1037,3204,2030,2061,1012,1012,102],[101,1030,26219,12083,2581,2683,2057,2442,2031,2042,...,14406,3406,10258,20668,1001,3582,2705,22494,5603,102],[101,2025,3110,2039,2000,11968,2651,1012,1001,6911,2075,102],[101,19387,1030,6506,24322,2015,1024,2342,1037,3835,...,2053,1044,18863,1012,1012,1012,2151,15690,1029,102],[101,2051,2000,2131,2070,3637,1012,2042,2041,2013,1018,9737,6229,2085,1012,102],...,[101,1001,2125,4179,2005,2247,2051,1012,1012,1012,2672,2025,2127,4826,102],[101,19387,1030,16266,1035,6520,1024,1041,11265,2213,...,25550,2099,1040,1024,2149,14691,7898,4430,2015,102],[101,1030,2358,3089,6292,2288,3067,1999,22861,2102,...,7622,1012,2106,2017,2131,21800,11486,1016,1029,102],[101,7243,14644,7507,26989,6632,4570,1012,4012,1024,...,2978,1012,1048,2100,1013,20704,2226,2683,6820,102],[101,3203,23332,2003,2488,1996,4833,2051,2821,3336,1006,1024,102],[101,2179,2070,2428,6057,4620,1997,3083,2095,1024,...,2183,2000,2404,2068,2006,9130,2085,999,999,102],[101,1030,2715,23298,1030,26234,19847,8525,4246,2054,...,3435,2814,1010,1998,2018,2019,6429,2051,1012,102],[101,2613,2100,11471,21526,2091,16525,2618,8038,2100,...,1024,6888,1024,11471,1011,1035,1011,5292,999,102],[101,1030,2377,11431,27914,2050,5292,3270,2009,1005,...,1996,21271,3085,2015,2008,12595,2066,16833,1012,102],[101,2085,2026,9450,2758,2008,2043,2002,11214,2033,...,1045,1005,2222,11997,999,999,8840,2140,1060,102]]]\n",
              " token_type_ids: [[[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],...,[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0]]]\n",
              " attention_mask: [[[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],...,[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,...,1,1,1,1,1,1,1,1,1,1]]]\n",
              " labels: [[[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],...,[-100,0,0,0,0,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,5,11,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,1,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100],[-100,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,-100]]]}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "tokenized_datasets.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "C7NbUg9Mc2Ma"
      },
      "outputs": [],
      "source": [
        "# Create Dataset Dictionary\n",
        "\n",
        "\n",
        "#\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "96FZr9RDc2Mb"
      },
      "outputs": [],
      "source": [
        "model_name = \"electra_small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQwZ3PQqc2Mb",
        "outputId": "758b1eaf-b3fe-4403-a1c1-d6474cda67be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(f\"{model_name}-finetuned-{task}\",\n",
        "              #  output_dir=tmp_dir,\n",
        "                learning_rate=2e-5,\n",
        "                per_device_train_batch_size=8,\n",
        "                per_device_eval_batch_size=8,\n",
        "                num_train_epochs=10,\n",
        "                #logging_dir=os.path.join(tmp_dir, \"logs\"),\n",
        "                logging_steps=max(1, len(train_data)//8//4),\n",
        "                save_steps=max(1, len(train_data)//48//4),\n",
        "                save_total_limit=2,              \n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=max(1, len(train_data)//8//4),\n",
        "                seed=42\n",
        "                \n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Vr2aDJEOc2Mb"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_datasets[\"train\"],           # training dataset\n",
        "    eval_dataset=tokenized_datasets[\"val\"],            # evaluation dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wFiQaApYc2Mb",
        "outputId": "9c966b2b-b773-4473-d937-5a057d4b40f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1916\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2400\n",
            "  Number of trainable parameters = 13486349\n",
            "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2400' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2400/2400 05:28, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.207100</td>\n",
              "      <td>0.379893</td>\n",
              "      <td>0.953661</td>\n",
              "      <td>0.971482</td>\n",
              "      <td>0.962489</td>\n",
              "      <td>0.955798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.342500</td>\n",
              "      <td>0.301963</td>\n",
              "      <td>0.953661</td>\n",
              "      <td>0.971482</td>\n",
              "      <td>0.962489</td>\n",
              "      <td>0.955798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.290400</td>\n",
              "      <td>0.288304</td>\n",
              "      <td>0.953661</td>\n",
              "      <td>0.971482</td>\n",
              "      <td>0.962489</td>\n",
              "      <td>0.955798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.297900</td>\n",
              "      <td>0.283285</td>\n",
              "      <td>0.953661</td>\n",
              "      <td>0.971482</td>\n",
              "      <td>0.962489</td>\n",
              "      <td>0.955798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.298300</td>\n",
              "      <td>0.270466</td>\n",
              "      <td>0.953661</td>\n",
              "      <td>0.971482</td>\n",
              "      <td>0.962489</td>\n",
              "      <td>0.955798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.226100</td>\n",
              "      <td>0.233949</td>\n",
              "      <td>0.953661</td>\n",
              "      <td>0.971482</td>\n",
              "      <td>0.962489</td>\n",
              "      <td>0.955798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>0.261300</td>\n",
              "      <td>0.215590</td>\n",
              "      <td>0.954103</td>\n",
              "      <td>0.971932</td>\n",
              "      <td>0.962935</td>\n",
              "      <td>0.957198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.223800</td>\n",
              "      <td>0.213161</td>\n",
              "      <td>0.955282</td>\n",
              "      <td>0.973133</td>\n",
              "      <td>0.964125</td>\n",
              "      <td>0.958597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.218300</td>\n",
              "      <td>0.212084</td>\n",
              "      <td>0.952262</td>\n",
              "      <td>0.970056</td>\n",
              "      <td>0.961077</td>\n",
              "      <td>0.956461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.204500</td>\n",
              "      <td>0.200413</td>\n",
              "      <td>0.956157</td>\n",
              "      <td>0.973809</td>\n",
              "      <td>0.964902</td>\n",
              "      <td>0.960586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.186900</td>\n",
              "      <td>0.194461</td>\n",
              "      <td>0.958115</td>\n",
              "      <td>0.975084</td>\n",
              "      <td>0.966525</td>\n",
              "      <td>0.962060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>0.172000</td>\n",
              "      <td>0.193841</td>\n",
              "      <td>0.953216</td>\n",
              "      <td>0.970957</td>\n",
              "      <td>0.962005</td>\n",
              "      <td>0.958008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>0.176500</td>\n",
              "      <td>0.188320</td>\n",
              "      <td>0.956419</td>\n",
              "      <td>0.973358</td>\n",
              "      <td>0.964814</td>\n",
              "      <td>0.960586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>826</td>\n",
              "      <td>0.181900</td>\n",
              "      <td>0.194058</td>\n",
              "      <td>0.956094</td>\n",
              "      <td>0.970732</td>\n",
              "      <td>0.963357</td>\n",
              "      <td>0.958818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>885</td>\n",
              "      <td>0.130100</td>\n",
              "      <td>0.189315</td>\n",
              "      <td>0.958300</td>\n",
              "      <td>0.974409</td>\n",
              "      <td>0.966287</td>\n",
              "      <td>0.962060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>944</td>\n",
              "      <td>0.199100</td>\n",
              "      <td>0.181294</td>\n",
              "      <td>0.959615</td>\n",
              "      <td>0.973659</td>\n",
              "      <td>0.966586</td>\n",
              "      <td>0.961986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1003</td>\n",
              "      <td>0.148100</td>\n",
              "      <td>0.181555</td>\n",
              "      <td>0.960077</td>\n",
              "      <td>0.972758</td>\n",
              "      <td>0.966376</td>\n",
              "      <td>0.961397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1062</td>\n",
              "      <td>0.155100</td>\n",
              "      <td>0.179857</td>\n",
              "      <td>0.960861</td>\n",
              "      <td>0.974634</td>\n",
              "      <td>0.967699</td>\n",
              "      <td>0.962870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1121</td>\n",
              "      <td>0.149800</td>\n",
              "      <td>0.179825</td>\n",
              "      <td>0.961245</td>\n",
              "      <td>0.973508</td>\n",
              "      <td>0.967338</td>\n",
              "      <td>0.962060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.168900</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.961655</td>\n",
              "      <td>0.973058</td>\n",
              "      <td>0.967323</td>\n",
              "      <td>0.961691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1239</td>\n",
              "      <td>0.141600</td>\n",
              "      <td>0.179382</td>\n",
              "      <td>0.962035</td>\n",
              "      <td>0.973659</td>\n",
              "      <td>0.967812</td>\n",
              "      <td>0.962355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1298</td>\n",
              "      <td>0.133300</td>\n",
              "      <td>0.181366</td>\n",
              "      <td>0.959289</td>\n",
              "      <td>0.972608</td>\n",
              "      <td>0.965903</td>\n",
              "      <td>0.961323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1357</td>\n",
              "      <td>0.139500</td>\n",
              "      <td>0.177522</td>\n",
              "      <td>0.961014</td>\n",
              "      <td>0.973058</td>\n",
              "      <td>0.966999</td>\n",
              "      <td>0.961839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1416</td>\n",
              "      <td>0.134300</td>\n",
              "      <td>0.178847</td>\n",
              "      <td>0.960157</td>\n",
              "      <td>0.972983</td>\n",
              "      <td>0.966528</td>\n",
              "      <td>0.961544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>0.147500</td>\n",
              "      <td>0.175263</td>\n",
              "      <td>0.960231</td>\n",
              "      <td>0.973058</td>\n",
              "      <td>0.966602</td>\n",
              "      <td>0.961691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1534</td>\n",
              "      <td>0.124400</td>\n",
              "      <td>0.176860</td>\n",
              "      <td>0.961233</td>\n",
              "      <td>0.973208</td>\n",
              "      <td>0.967184</td>\n",
              "      <td>0.961912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1593</td>\n",
              "      <td>0.127800</td>\n",
              "      <td>0.179146</td>\n",
              "      <td>0.961518</td>\n",
              "      <td>0.973208</td>\n",
              "      <td>0.967328</td>\n",
              "      <td>0.961986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1652</td>\n",
              "      <td>0.138600</td>\n",
              "      <td>0.174783</td>\n",
              "      <td>0.961145</td>\n",
              "      <td>0.972758</td>\n",
              "      <td>0.966917</td>\n",
              "      <td>0.961470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1711</td>\n",
              "      <td>0.141100</td>\n",
              "      <td>0.174897</td>\n",
              "      <td>0.960951</td>\n",
              "      <td>0.973283</td>\n",
              "      <td>0.967078</td>\n",
              "      <td>0.961839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>0.123200</td>\n",
              "      <td>0.174823</td>\n",
              "      <td>0.960694</td>\n",
              "      <td>0.972158</td>\n",
              "      <td>0.966392</td>\n",
              "      <td>0.961028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1829</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.173796</td>\n",
              "      <td>0.962089</td>\n",
              "      <td>0.973208</td>\n",
              "      <td>0.967617</td>\n",
              "      <td>0.961986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1888</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>0.174049</td>\n",
              "      <td>0.962337</td>\n",
              "      <td>0.974109</td>\n",
              "      <td>0.968187</td>\n",
              "      <td>0.962870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1947</td>\n",
              "      <td>0.117800</td>\n",
              "      <td>0.178730</td>\n",
              "      <td>0.962120</td>\n",
              "      <td>0.974034</td>\n",
              "      <td>0.968040</td>\n",
              "      <td>0.962797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2006</td>\n",
              "      <td>0.116000</td>\n",
              "      <td>0.176533</td>\n",
              "      <td>0.961829</td>\n",
              "      <td>0.973884</td>\n",
              "      <td>0.967819</td>\n",
              "      <td>0.962649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2065</td>\n",
              "      <td>0.127100</td>\n",
              "      <td>0.175027</td>\n",
              "      <td>0.960828</td>\n",
              "      <td>0.971932</td>\n",
              "      <td>0.966348</td>\n",
              "      <td>0.960955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2124</td>\n",
              "      <td>0.117200</td>\n",
              "      <td>0.173961</td>\n",
              "      <td>0.962400</td>\n",
              "      <td>0.973884</td>\n",
              "      <td>0.968108</td>\n",
              "      <td>0.962797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2183</td>\n",
              "      <td>0.115700</td>\n",
              "      <td>0.176246</td>\n",
              "      <td>0.962252</td>\n",
              "      <td>0.973734</td>\n",
              "      <td>0.967959</td>\n",
              "      <td>0.962649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2242</td>\n",
              "      <td>0.114400</td>\n",
              "      <td>0.174781</td>\n",
              "      <td>0.961790</td>\n",
              "      <td>0.972833</td>\n",
              "      <td>0.967280</td>\n",
              "      <td>0.961839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2301</td>\n",
              "      <td>0.113800</td>\n",
              "      <td>0.174279</td>\n",
              "      <td>0.961556</td>\n",
              "      <td>0.972308</td>\n",
              "      <td>0.966902</td>\n",
              "      <td>0.961323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>0.116300</td>\n",
              "      <td>0.174115</td>\n",
              "      <td>0.961704</td>\n",
              "      <td>0.972458</td>\n",
              "      <td>0.967051</td>\n",
              "      <td>0.961397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-9\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-9/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-9/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-9/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-9/special_tokens_map.json\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-18\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-18/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-18/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-18/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-18/special_tokens_map.json\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-27\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-27/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-27/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-27/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-27/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-9] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-36\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-36/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-36/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-36/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-36/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-18] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-45\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-45/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-45/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-45/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-45/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-27] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-54\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-54/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-54/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-54/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-54/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-36] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-63\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-63/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-63/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-63/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-63/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-45] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-72\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-72/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-72/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-72/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-72/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-54] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-81\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-81/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-81/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-81/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-81/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-63] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-90\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-90/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-90/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-90/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-72] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-99\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-99/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-99/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-99/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-99/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-81] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-108\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-108/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-108/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-108/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-108/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-90] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-117\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-117/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-117/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-117/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-117/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-99] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-126\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-126/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-126/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-126/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-126/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-108] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-135\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-135/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-135/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-135/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-135/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-117] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-144\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-144/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-144/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-144/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-144/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-126] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-153\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-153/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-153/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-153/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-153/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-135] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-162\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-162/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-162/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-162/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-162/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-144] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-171\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-171/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-171/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-171/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-171/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-153] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-180\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-180/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-180/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-180/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-162] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-189\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-189/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-189/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-189/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-189/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-171] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-198\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-198/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-198/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-198/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-198/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-180] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-207\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-207/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-207/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-207/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-207/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-189] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-216\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-216/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-216/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-216/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-216/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-198] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-225\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-225/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-225/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-225/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-225/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-207] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-234\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-234/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-234/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-234/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-234/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-216] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-243\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-243/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-243/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-243/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-243/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-225] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-252\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-252/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-252/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-252/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-252/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-234] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-261\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-261/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-261/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-261/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-261/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-243] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-270\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-270/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-270/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-270/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-252] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-279\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-279/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-279/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-279/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-279/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-261] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-288\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-288/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-288/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-288/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-288/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-270] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-297\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-297/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-297/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-297/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-297/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-279] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-306\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-306/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-306/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-306/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-306/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-288] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-315\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-315/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-315/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-315/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-315/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-297] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-324\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-324/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-324/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-324/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-324/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-306] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-333\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-333/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-333/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-333/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-333/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-315] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-342\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-342/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-342/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-342/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-342/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-324] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-351\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-351/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-351/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-351/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-351/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-333] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-360\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-360/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-360/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-360/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-360/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-342] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-369\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-369/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-369/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-369/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-369/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-351] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-378\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-378/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-378/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-378/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-378/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-360] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-387\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-387/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-387/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-387/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-387/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-369] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-396\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-396/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-396/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-396/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-396/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-378] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-405\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-405/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-405/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-405/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-405/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-387] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-414\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-414/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-414/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-414/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-414/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-396] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-423\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-423/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-423/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-423/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-423/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-405] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-432\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-432/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-432/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-432/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-432/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-414] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-441\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-441/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-441/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-441/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-441/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-423] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-450\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-450/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-450/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-450/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-450/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-432] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-459\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-459/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-459/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-459/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-459/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-441] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-468\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-468/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-468/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-468/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-468/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-450] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-477\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-477/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-477/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-477/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-477/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-459] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-486\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-486/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-486/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-486/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-486/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-468] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-495\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-495/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-495/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-495/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-495/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-477] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-504\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-504/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-504/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-504/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-504/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-486] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-513\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-513/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-513/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-513/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-513/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-495] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-522\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-522/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-522/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-522/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-522/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-504] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-531\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-531/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-531/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-531/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-531/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-513] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-540\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-540/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-540/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-540/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-540/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-522] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-549\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-549/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-549/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-549/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-549/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-531] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-558\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-558/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-558/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-558/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-558/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-540] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-567\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-567/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-567/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-567/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-567/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-549] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-576\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-576/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-576/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-576/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-576/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-558] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-585\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-585/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-585/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-585/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-585/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-567] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-594\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-594/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-594/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-594/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-594/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-576] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-603\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-603/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-603/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-603/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-603/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-585] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-612\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-612/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-612/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-612/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-612/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-594] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-621\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-621/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-621/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-621/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-621/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-603] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-630\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-630/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-630/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-630/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-630/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-612] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-639\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-639/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-639/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-639/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-639/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-621] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-648\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-648/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-648/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-648/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-648/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-630] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-657\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-657/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-657/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-657/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-657/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-639] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-666\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-666/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-666/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-666/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-666/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-648] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-675\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-675/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-675/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-675/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-675/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-657] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-684\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-684/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-684/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-684/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-684/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-666] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-693\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-693/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-693/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-693/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-693/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-675] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-702\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-702/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-702/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-702/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-702/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-684] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-711\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-711/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-711/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-711/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-711/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-693] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-720\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-720/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-720/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-720/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-720/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-702] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-729\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-729/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-729/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-729/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-729/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-711] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-738\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-738/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-738/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-738/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-738/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-720] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-747\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-747/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-747/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-747/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-747/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-729] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-756\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-756/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-756/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-756/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-756/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-738] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-765\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-765/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-765/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-765/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-765/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-747] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-774\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-774/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-774/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-774/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-774/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-756] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-783\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-783/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-783/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-783/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-783/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-765] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-792\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-792/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-792/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-792/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-792/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-774] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-801\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-801/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-801/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-801/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-801/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-783] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-810\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-810/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-810/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-810/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-810/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-792] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-819\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-819/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-819/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-819/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-819/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-801] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-828\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-828/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-828/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-828/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-828/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-810] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-837\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-837/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-837/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-837/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-837/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-819] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-846\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-846/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-846/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-846/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-846/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-828] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-855\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-855/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-855/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-855/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-855/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-837] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-864\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-864/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-864/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-864/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-864/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-846] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-873\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-873/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-873/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-873/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-873/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-855] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-882\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-882/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-882/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-882/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-882/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-864] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-891\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-891/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-891/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-891/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-891/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-873] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-900\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-900/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-900/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-900/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-900/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-882] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-909\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-909/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-909/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-909/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-909/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-891] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-918\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-918/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-918/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-918/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-918/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-900] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-927\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-927/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-927/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-927/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-927/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-909] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-936\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-936/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-936/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-936/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-936/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-918] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-945\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-945/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-945/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-945/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-945/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-927] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-954\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-954/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-954/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-954/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-954/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-936] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-963\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-963/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-963/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-963/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-963/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-945] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-972\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-972/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-972/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-972/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-972/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-954] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-981\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-981/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-981/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-981/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-981/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-963] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-990\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-990/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-990/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-990/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-990/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-972] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-999\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-999/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-999/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-999/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-999/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-981] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1008\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1008/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1008/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1008/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1008/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-990] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1017\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1017/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1017/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1017/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1017/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-999] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1026\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1026/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1026/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1026/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1026/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1008] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1035\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1035/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1035/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1035/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1035/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1017] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1044\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1044/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1044/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1044/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1044/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1026] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1053\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1053/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1053/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1053/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1053/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1035] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1062\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1062/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1062/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1062/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1062/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1044] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1071\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1071/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1071/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1071/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1071/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1053] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1080\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1080/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1080/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1080/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1080/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1062] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1089\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1089/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1089/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1089/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1089/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1071] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1098\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1098/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1098/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1098/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1098/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1080] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1107\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1107/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1107/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1107/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1107/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1089] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1116\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1116/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1116/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1116/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1116/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1098] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1125\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1125/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1125/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1125/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1125/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1107] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1134\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1134/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1134/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1134/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1134/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1116] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1143\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1143/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1143/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1143/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1143/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1125] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1152\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1152/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1152/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1152/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1152/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1134] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1161\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1161/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1161/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1161/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1161/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1143] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1170\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1170/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1170/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1170/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1170/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1152] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1179\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1179/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1179/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1179/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1179/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1161] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1188\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1188/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1188/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1188/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1188/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1170] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1197\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1197/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1197/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1197/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1197/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1179] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1206\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1206/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1206/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1206/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1206/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1188] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1215\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1215/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1215/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1215/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1215/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1197] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1224\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1224/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1224/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1224/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1224/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1206] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1233\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1233/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1233/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1233/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1233/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1215] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1242\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1242/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1242/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1242/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1242/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1224] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1251\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1251/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1251/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1251/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1251/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1233] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1260\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1260/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1260/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1260/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1260/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1242] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1269\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1269/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1269/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1269/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1269/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1251] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1278\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1278/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1278/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1278/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1278/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1260] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1287\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1287/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1287/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1287/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1287/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1269] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1296\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1296/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1296/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1296/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1296/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1278] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1305\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1305/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1305/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1305/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1305/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1287] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1314\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1314/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1314/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1314/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1314/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1296] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1323\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1323/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1323/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1323/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1323/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1305] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1332\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1332/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1332/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1332/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1332/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1314] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1341\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1341/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1341/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1341/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1341/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1323] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1350\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1350/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1350/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1350/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1350/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1332] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1359\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1359/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1359/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1359/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1359/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1341] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1368\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1368/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1368/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1368/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1368/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1350] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1377\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1377/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1377/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1377/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1377/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1359] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1386\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1386/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1386/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1386/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1386/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1368] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1395\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1395/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1395/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1395/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1395/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1377] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1404\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1404/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1404/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1404/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1404/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1386] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1413\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1413/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1413/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1413/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1413/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1395] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1422\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1422/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1422/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1422/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1422/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1404] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1431\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1431/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1431/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1431/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1431/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1413] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1440\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1440/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1440/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1440/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1440/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1422] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1449\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1449/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1449/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1449/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1449/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1431] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1458\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1458/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1458/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1458/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1458/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1440] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1467\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1467/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1467/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1467/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1467/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1449] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1476\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1476/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1476/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1476/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1476/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1458] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1485\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1485/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1485/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1485/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1485/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1467] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1494\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1494/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1494/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1494/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1494/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1476] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1503\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1503/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1503/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1503/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1503/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1485] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1512\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1512/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1512/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1512/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1512/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1494] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1521\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1521/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1521/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1521/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1521/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1503] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1530\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1530/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1530/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1530/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1530/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1512] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1539\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1539/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1539/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1539/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1539/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1521] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1548\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1548/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1548/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1548/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1548/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1530] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1557\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1557/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1557/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1557/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1557/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1539] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1566\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1566/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1566/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1566/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1566/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1548] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1575\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1575/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1575/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1575/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1575/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1557] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1584\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1584/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1584/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1584/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1584/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1566] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1593\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1593/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1593/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1593/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1593/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1575] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1602\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1602/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1602/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1602/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1602/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1584] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1611\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1611/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1611/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1611/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1611/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1593] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1620\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1620/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1620/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1620/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1620/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1602] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1629\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1629/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1629/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1629/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1629/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1611] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1638\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1638/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1638/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1638/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1638/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1620] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1647\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1647/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1647/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1647/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1647/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1629] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1656\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1656/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1656/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1656/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1656/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1638] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1665\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1665/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1665/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1665/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1665/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1647] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1674\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1674/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1674/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1674/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1674/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1656] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1683\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1683/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1683/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1683/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1683/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1665] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1692\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1692/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1692/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1692/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1692/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1674] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1701\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1701/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1701/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1701/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1701/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1683] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1710\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1710/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1710/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1710/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1710/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1692] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1719\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1719/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1719/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1719/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1719/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1701] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1728\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1728/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1728/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1728/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1728/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1710] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1737\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1737/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1737/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1737/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1737/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1719] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1746\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1746/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1746/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1746/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1746/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1728] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1755\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1755/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1755/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1755/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1755/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1737] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1764\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1764/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1764/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1764/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1764/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1746] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1773\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1773/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1773/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1773/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1773/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1755] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1782\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1782/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1782/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1782/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1782/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1764] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1791\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1791/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1791/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1791/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1791/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1773] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1800\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1800/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1800/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1800/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1800/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1782] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1809\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1809/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1809/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1809/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1809/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1791] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1818\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1818/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1818/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1818/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1818/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1800] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1827\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1827/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1827/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1827/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1827/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1809] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1836\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1836/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1836/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1836/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1836/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1818] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1845\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1845/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1845/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1845/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1845/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1827] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1854\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1854/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1854/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1854/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1854/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1836] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1863\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1863/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1863/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1863/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1863/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1845] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1872\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1872/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1872/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1872/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1872/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1854] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1881\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1881/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1881/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1881/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1881/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1863] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1890\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1890/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1890/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1890/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1890/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1872] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1899\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1899/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1899/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1899/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1899/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1881] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1908\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1908/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1908/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1908/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1908/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1890] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1917\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1917/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1917/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1917/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1917/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1899] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1926\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1926/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1926/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1926/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1926/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1908] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1935\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1935/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1935/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1935/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1935/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1917] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1944\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1944/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1944/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1944/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1944/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1926] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1953\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1953/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1953/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1953/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1953/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1935] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1962\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1962/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1962/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1962/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1962/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1944] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1971\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1971/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1971/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1971/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1971/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1953] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1980\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1980/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1980/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1980/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1980/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1962] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1989\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1989/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1989/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1989/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1989/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1971] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-1998\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-1998/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-1998/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-1998/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-1998/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1980] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2007\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2007/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2007/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2007/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2007/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1989] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2016\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2016/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2016/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2016/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2016/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-1998] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2025\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2025/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2025/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2025/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2025/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2007] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2034\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2034/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2034/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2034/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2034/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2016] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2043\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2043/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2043/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2043/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2043/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2025] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2052\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2052/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2052/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2052/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2052/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2034] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2061\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2061/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2061/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2061/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2061/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2043] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2070\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2070/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2070/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2070/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2070/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2052] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2079\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2079/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2079/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2079/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2079/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2061] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2088\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2088/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2088/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2088/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2088/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2070] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2097\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2097/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2097/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2097/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2097/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2079] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2106\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2106/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2106/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2106/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2106/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2088] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2115\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2115/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2115/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2115/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2115/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2097] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2124\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2124/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2124/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2124/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2124/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2106] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2133\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2133/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2133/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2133/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2133/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2115] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2142\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2142/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2142/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2142/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2142/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2124] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2151\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2151/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2151/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2151/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2151/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2133] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2160\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2160/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2160/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2160/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2160/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2142] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2169\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2169/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2169/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2169/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2169/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2151] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2178\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2178/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2178/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2178/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2178/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2160] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2187\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2187/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2187/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2187/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2187/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2169] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2196\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2196/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2196/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2196/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2196/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2178] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2205\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2205/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2205/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2205/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2205/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2187] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2214\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2214/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2214/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2214/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2214/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2196] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2223\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2223/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2223/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2223/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2223/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2205] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2232\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2232/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2232/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2232/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2232/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2214] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2241\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2241/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2241/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2241/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2241/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2223] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2250\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2250/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2250/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2250/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2250/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2232] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2259\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2259/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2259/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2259/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2259/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2241] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2268\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2268/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2268/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2268/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2268/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2250] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2277\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2277/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2277/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2277/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2277/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2259] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2286\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2286/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2286/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2286/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2286/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2268] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2295\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2295/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2295/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2295/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2295/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2277] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2304\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2304/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2304/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2304/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2304/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2286] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2313\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2313/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2313/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2313/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2313/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2295] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2322\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2322/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2322/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2322/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2322/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2304] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2331\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2331/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2331/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2331/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2331/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2313] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2340\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2340/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2340/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2340/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2340/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2322] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2349\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2349/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2349/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2349/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2349/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2331] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2358\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2358/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2358/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2358/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2358/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2340] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2367\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2367/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2367/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2367/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2367/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2349] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2376\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2376/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2376/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2376/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2376/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2358] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2385\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2385/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2385/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2385/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2385/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2367] due to args.save_total_limit\n",
            "Saving model checkpoint to electra_small-finetuned-ner/checkpoint-2394\n",
            "Configuration saved in electra_small-finetuned-ner/checkpoint-2394/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/checkpoint-2394/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/checkpoint-2394/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/checkpoint-2394/special_tokens_map.json\n",
            "Deleting older checkpoint [electra_small-finetuned-ner/checkpoint-2376] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2400, training_loss=0.1931462961435318, metrics={'train_runtime': 330.7379, 'train_samples_per_second': 57.931, 'train_steps_per_second': 7.257, 'total_flos': 54137448445968.0, 'train_loss': 0.1931462961435318, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "SjJK8dO2c2Mc",
        "outputId": "eb67a1c7-8653-4c11-d84e-b1d9c0548fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id. If tokens, id are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 479\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.17414917051792145,\n",
              " 'eval_precision': 0.9614928030865113,\n",
              " 'eval_recall': 0.9725328330206379,\n",
              " 'eval_f1': 0.9669813080625302,\n",
              " 'eval_accuracy': 0.9615441284809194,\n",
              " 'eval_runtime': 1.1582,\n",
              " 'eval_samples_per_second': 413.571,\n",
              " 'eval_steps_per_second': 51.804,\n",
              " 'epoch': 10.0}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-eXQ-amc2Mc",
        "outputId": "e5a1b064-e694-4e35-cd04-e967b0b0fbb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to electra_small-finetuned-ner\n",
            "Configuration saved in electra_small-finetuned-ner/config.json\n",
            "Model weights saved in electra_small-finetuned-ner/pytorch_model.bin\n",
            "tokenizer config file saved in electra_small-finetuned-ner/tokenizer_config.json\n",
            "Special tokens file saved in electra_small-finetuned-ner/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/electra_small-finetuned-ner/pytorch_model.bin') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fWZXTTS7nOx6",
        "outputId": "d8b1ae80-12f6-4da6-8d89-37a7f3658051"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0dbef7f5-87c2-452d-b6a6-1a056bc59bb6\", \"pytorch_model.bin\", 54016941)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/electra_small-finetuned-ner/pytorch_model.bin"
      ],
      "metadata": {
        "id": "WuFAtWlxjZBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b3e7f9c22514158bd98648462b7f5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aec8089482f94908a338eec012f929f1",
              "IPY_MODEL_41403e95d81f4ddc9e98025478c0981c",
              "IPY_MODEL_99fa2e7ec5e448e0840ba7ccd0365d7c"
            ],
            "layout": "IPY_MODEL_c625f6dabe2440c0b9c948301e9cdd2f"
          }
        },
        "aec8089482f94908a338eec012f929f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae9e256d7ff47c1a46d0a92b0b59acc",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1f148e68234cc5a8ef7780b4077324",
            "value": "Downloading builder script: "
          }
        },
        "41403e95d81f4ddc9e98025478c0981c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63feafd108db45e6814f0d368ba675f9",
            "max": 2472,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_868d56759199411297904f861f962b68",
            "value": 2472
          }
        },
        "99fa2e7ec5e448e0840ba7ccd0365d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_234bc2ff31af41698a5a984e48f11fc8",
            "placeholder": "​",
            "style": "IPY_MODEL_752017550ea04c82890eee47247bd3f2",
            "value": " 6.33k/? [00:00&lt;00:00, 117kB/s]"
          }
        },
        "c625f6dabe2440c0b9c948301e9cdd2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cae9e256d7ff47c1a46d0a92b0b59acc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1f148e68234cc5a8ef7780b4077324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63feafd108db45e6814f0d368ba675f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868d56759199411297904f861f962b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "234bc2ff31af41698a5a984e48f11fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752017550ea04c82890eee47247bd3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9988b2ab199549d8af175c7ab5cb794a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bedfe5103854e57adebf720143e21fd",
              "IPY_MODEL_6b6202d3fa5d462a8bca5ae3aa89c482",
              "IPY_MODEL_c0be6ed22ce7444fbf6c0acc8a3b8cd1"
            ],
            "layout": "IPY_MODEL_5ca3d45dc76b407db10066b0cf685cb1"
          }
        },
        "0bedfe5103854e57adebf720143e21fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c677a5eee241c08798fa0e4fec6133",
            "placeholder": "​",
            "style": "IPY_MODEL_cf9f3e0aa87b47a59c3cc684ddaa9b15",
            "value": " 50%"
          }
        },
        "6b6202d3fa5d462a8bca5ae3aa89c482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365b684eced14b149b00a223886a4866",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad929dd29e464817b8e9a53ed3aaf285",
            "value": 1
          }
        },
        "c0be6ed22ce7444fbf6c0acc8a3b8cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d33f82d50bb482f88a106252fdbf7cc",
            "placeholder": "​",
            "style": "IPY_MODEL_39f4f8e626ef4c8885d09c901fe52f17",
            "value": " 1/2 [00:00&lt;00:00,  3.24ba/s]"
          }
        },
        "5ca3d45dc76b407db10066b0cf685cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c677a5eee241c08798fa0e4fec6133": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf9f3e0aa87b47a59c3cc684ddaa9b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "365b684eced14b149b00a223886a4866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad929dd29e464817b8e9a53ed3aaf285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d33f82d50bb482f88a106252fdbf7cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f4f8e626ef4c8885d09c901fe52f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1695e6497ac1471780a914519e88b7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dda008d41825416da96d9d11930c5394",
              "IPY_MODEL_c3b618ccefb24449bf25c9ae8d3e3c19",
              "IPY_MODEL_45b010e5ee3e4ef4b089d5a0d4d26d4c"
            ],
            "layout": "IPY_MODEL_ee1b85ee15644e738351cfeb0c62efe1"
          }
        },
        "dda008d41825416da96d9d11930c5394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_200cf109007a4a1890c1de28e3a227e1",
            "placeholder": "​",
            "style": "IPY_MODEL_7aca39f9474841998e8133c6e5ff3450",
            "value": "  0%"
          }
        },
        "c3b618ccefb24449bf25c9ae8d3e3c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba9673ef17354844a6853a57ce3159b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab3d2c361cf9475eb31063cfc29c0a9b",
            "value": 0
          }
        },
        "45b010e5ee3e4ef4b089d5a0d4d26d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62052ebc800347c68169bf28cd647748",
            "placeholder": "​",
            "style": "IPY_MODEL_13ce58fc81da40be8e75b83a5089bc5f",
            "value": " 0/1 [00:00&lt;?, ?ba/s]"
          }
        },
        "ee1b85ee15644e738351cfeb0c62efe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200cf109007a4a1890c1de28e3a227e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aca39f9474841998e8133c6e5ff3450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba9673ef17354844a6853a57ce3159b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab3d2c361cf9475eb31063cfc29c0a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62052ebc800347c68169bf28cd647748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ce58fc81da40be8e75b83a5089bc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}